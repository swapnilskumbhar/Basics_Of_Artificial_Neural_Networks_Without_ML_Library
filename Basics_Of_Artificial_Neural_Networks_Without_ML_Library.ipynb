{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics Of Artificial Neural Networks Without ML Library\n",
    "We as species learnt many things, achieved many things. We were living in caves, hunting animals, walking naked and look at us now we are most advanced civilization that ever lived on this planet and may colonize other planets in near future. From invention of the wheel, farming to the invention of artificial intelligence we have achived great many things. Through all this journey humans have most biologically advanced,complex computer with humans and that is our brain. As our last invention we want to mimic same behaviour as our brain into the machines, so we started studying our own brain and it turned out that the neuron is the basic working unit of the brain and brain has almost 100 billion of them. We created the models of mathematics which will represent the behaviour/pattern observed in our brain. Those models are known as artificial neural networks. After experimenting with different approaches we have created network of neurons that have multiple layers that is known as deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is artificial neuron exactly ? \n",
    "If we draw the representation of the neuron it will be as follows,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Diagram-of-single-neuron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the equation of the output generated by the neuron. Weighted sum of all the input plus bias number is given to a function called as activation function and what activation function gives as the output is the final output of the neuron. The activation function is choosen according to the problem. For the problem we are trying to solve here we will use sigmoid as output function, whoes graph is given as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/sigmoid_function.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our weighted sum of the inputs plus bias is 0 then o/p of the function is 0.5 and as the input to the sigmoid function increases the o/p of the function gives a value which is close to one and if the input to the function is below zero the output is the value close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally the artifial neural network consists of layers of the above neurons. The layer which takes input is known as input layer similarly output layer gives the output. If you have output which is linear function of the input then those two layers are enough but if you have output which is non linear function of input then you should introduce the layers of neurons between the input layer and output layer. These layers are known as hidden layers and if one network have hidden more than one hidden layer then that network is known as deep neural network. If we have enough hidden layers then we can map any function using the neural network thus neural network follows the Universal Approximation Theorem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample structure of neural network is shown in below diagram,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/NeuralNetwork.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For above network we have 3 neurons in the input layers, 2 neurons in output layers and 4 neurons in hidden layer, every edge has some weight associated with it so we have (3 * 4) + (4 * 2) = 20 weighs plus 9 bias (each neuron has 1 bias) that we can adjust to map the output to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider that we have adjusted all the weights and biases properly. Upon giving the input through the input neurons that input is multiplied by the weights of the edges that they travel the bias is added and that value gets activated by the activation function of the hidden layer. then the same process repeated throughout the network and finally we get output through the output neurons. This all process is known as forward propagation through the network. So for taking the output from the input we forwardpropagate through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a scenario where we have not adjusted the weights of the network. All we have is the labeled data through which we have to update the weights. this process of updating the data from the data is known as learning. In neural networks learning is done by backpropagating through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So How those weights get activated ? \n",
    "For that we use the gradient descent algorithm. For basics of gradient descent algorithm go through <a href=\"https://github.com/swapnilskumbhar/Linear_Regresision_using_Gradient_Descent\">this repository </a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we use the concept of calculus known as derivatives. If we take the derivative of an function at a specific point then derivative gives us the slope of the line which is tanget to that function at that that point. So if we take the derivative of the error function by putting the values of current parameters of our networks then we get the slope of the line which is tangent to the error function and by using that slope, we know whether to increase or decrease the values of our parameters in order to decrease the error value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets take an example, We will try to model a basic 3 variable XOR gate for that we will consider the same network as shown in above diagram except it will have only one output neuron. we will train on the 7 inputs and see the output the network gives in rest case, so lets start coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 1]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]]\n",
      "[[0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 1]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1],[1,1,0]])\n",
    "output_labels = np.array([[0],[1],[1],[0],[1],[0],[0]])\n",
    "\n",
    "print(input_data)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the sigmoid function as our activation function. For forward propagation we will use plane sigmoid function and for back propagation we will use the derivative to findout the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function forward propagation\n",
    "def forwardPropagate(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "# sigmoid derivative backward propagation\n",
    "def backPropagate(x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the weights in matrix for easier calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54630335 0.24506107 0.71869322 0.71683982]\n",
      " [0.0171108  0.39839738 0.22501579 0.20357938]\n",
      " [0.90015134 0.21743937 0.93926823 0.05391663]]\n",
      "[[0.29412371]\n",
      " [0.94795666]\n",
      " [0.39918286]\n",
      " [0.4951012 ]]\n"
     ]
    }
   ],
   "source": [
    "weights_layer0_1 = np.random.random((3,4))\n",
    "weights_layer1_2 = np.random.random((4,1))\n",
    "\n",
    "print(weights_layer0_1)\n",
    "print(weights_layer1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now core logic \n",
    "1. Forward propagating to findout the output of each layer\n",
    "2. Calculating error at each layer\n",
    "3. Updating the weights through backpropagation\n",
    "\n",
    "repeating above steps is known as training of the network so lets begin training,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.5476388795162251\n",
      "Error:0.010861392744054458\n",
      "Error:0.007492419365711384\n",
      "Error:0.006063064962322255\n",
      "Error:0.005224070724556623\n",
      "Error:0.004656223772483918\n"
     ]
    }
   ],
   "source": [
    "for i in range(60000):\n",
    "    #forward Propagate to findout the output of the layer\n",
    "    layer0 = input_data\n",
    "    #Output of the layer1 is given to the 2nd layer as input\n",
    "    layer1 = forwardPropagate(np.dot(layer0,weights_layer0_1))\n",
    "    #Output of the network\n",
    "    layer2 = forwardPropagate(np.dot(layer1,weights_layer1_2))\n",
    "    #Calculating the error by subtracting the labels and predicted the values\n",
    "    error = output_labels - layer2\n",
    "    #Backpropagation of output to the input\n",
    "    layer2_gradient = error*backPropagate(layer2)\n",
    "    #Backpropagation from 2nd layer to 1st layer\n",
    "    #Calculating the error of the layer1\n",
    "    layer1_error = layer2_gradient.dot(weights_layer1_2.T)\n",
    "    #calculating the gradient\n",
    "    layer1_gradient = layer1_error * backPropagate(layer1)\n",
    "    if (i% 10000) == 0:\n",
    "        print (\"Error:\" + str(np.mean(np.abs(error))))\n",
    "    #update the weights using the gradients \n",
    "    weights_layer1_2 += layer1.T.dot(layer2_gradient) \n",
    "    weights_layer0_1 += layer0.T.dot(layer1_gradient)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that error is decreasing as we iterate more and more through the dataset.\n",
    "Now the we have updated the weights of the network lets test the network for the remaining input which was not in the data which we have used for training, i.e. [1,1,1] if the output of the network is 1 then we should say that network has given correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the network is [[0.99999243]]\n"
     ]
    }
   ],
   "source": [
    "#For the new Input\n",
    "test_data = [[1,1,1]]\n",
    "layer1_op = forwardPropagate(np.dot(test_data,weights_layer0_1))\n",
    "layer2_op = forwardPropagate(np.dot(layer1_op,weights_layer1_2))\n",
    "\n",
    "print(\"Output of the network is\",layer2_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting the output we have to forward propagate through network 1 time.\n",
    "We can see the network has given the correct output as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that we discussed in this article is known as the feed forward network which is generally used to map relation between output and input.<br/>Almost every tech company is using neural network based algorithms to solve their problmes. Becuase the neural networks if given enough data and computing power outperforms almost all the machine learning techniques and we need less engineering design choices. also NN can approximate any function if it has enough parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
